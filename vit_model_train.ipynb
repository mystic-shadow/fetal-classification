{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzQiI5Sx3Rgd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MelSpecDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "\n",
        "        self.samples = []\n",
        "        self.label_map = {'mother': 0, 'fetus': 1}\n",
        "\n",
        "        for label_name in ['mother', 'fetus']:\n",
        "            folder = os.path.join(data_dir, label_name)\n",
        "            files = glob(os.path.join(folder, \"*.npy\"))\n",
        "            for f in files:\n",
        "                self.samples.append((f, self.label_map[label_name]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        mel = np.load(path)\n",
        "        mel = torch.tensor(mel, dtype=torch.float32)\n",
        "\n",
        "        mel = (mel - mel.mean()) / (mel.std() + 1e-6)\n",
        "\n",
        "        return mel.unsqueeze(0), torch.tensor(label, dtype=torch.long)"
      ],
      "metadata": {
        "id": "gY5KX27B6JFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import MelSpecDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = MelSpecDataset(\"/content/drive/MyDrive/SUFHSDB/training_data\")\n",
        "test_ds = MelSpecDataset(\"/content/drive/MyDrive/SUFHSDB/testing_data\")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "o5Tp7sru6RdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT"
      ],
      "metadata": {
        "id": "t5TThFK96p2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit_pytorch"
      ],
      "metadata": {
        "id": "_dDSpFX96tjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vit_pytorch import SimpleViT\n",
        "\n",
        "vit_model = SimpleViT(\n",
        "    image_size = 128,\n",
        "    patch_size = 32,\n",
        "    num_classes = 2,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048\n",
        ")"
      ],
      "metadata": {
        "id": "DVoHO2K76lVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vit_model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "tIFLpFxa7Tj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y in tqdm(loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        x = torch.nn.functional.interpolate(x, size=(128,128), mode='bilinear')\n",
        "        x = x.repeat(1,3,1,1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "vsZUtRBM7YCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = torch.nn.functional.interpolate(x, size=(256,256), mode='bilinear')\n",
        "            x = x.repeat(1,3,1,1)\n",
        "\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            running_loss += loss.item() * x.size(0)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "lqw04JHN7c3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "\n",
        "history = {\n",
        "    \"train_accuracy\": [],\n",
        "    \"train_loss\": [],\n",
        "    \"test_accuracy\": [],\n",
        "    \"test_loss\": []\n",
        "}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    train_loss, train_acc = train_one_epoch(vit_model, train_loader)\n",
        "    test_loss, test_acc = evaluate(vit_model, test_loader)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    history[\"train_accuracy\"].append(train_acc)\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"test_accuracy\"].append(test_acc)\n",
        "    history[\"test_loss\"].append(test_loss)\n"
      ],
      "metadata": {
        "id": "5W70Nel57gc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(history)\n",
        "df.to_csv(\"/content/drive/MyDrive/SUFHSDB/vit_with_specaugment.csv\", index=False)"
      ],
      "metadata": {
        "id": "rpp1bCRc7pc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "8pmizm4k7zms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        x = torch.nn.functional.interpolate(x, size=(128,128), mode='bilinear')\n",
        "        x = x.repeat(1,3,1,1)\n",
        "\n",
        "        outputs =vit_model(x)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        y_true.extend(y.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())"
      ],
      "metadata": {
        "id": "QfDPeIid8p2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc = df[\"train_accuracy\"]\n",
        "val_acc = df[\"test_accuracy\"]\n",
        "train_loss = df[\"train_loss\"]\n",
        "val_loss = df[\"test_loss\"]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12,4))\n",
        "\n",
        "axs[0].plot(epochs, train_acc, label='Train Accuracy')\n",
        "axs[0].plot(epochs, val_acc, label='Validation Accuracy')\n",
        "axs[0].set_xlabel(\"Number of Epochs\", weight='bold')\n",
        "axs[0].set_ylabel(\"Accuracy\", weight='bold')\n",
        "axs[0].set_title(\"CaiT Model Accuracy\")\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "\n",
        "axs[1].plot(epochs, train_loss, label='Train Loss')\n",
        "axs[1].plot(epochs, val_loss, label='Validation Loss')\n",
        "axs[1].set_xlabel(\"Number of Epochs\", weight='bold')\n",
        "axs[1].set_ylabel(\"Loss\", weight='bold')\n",
        "axs[1].set_title(\"Model Loss\")\n",
        "axs[1].legend()\n",
        "axs[1].grid(True)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
        "            xticklabels=['Fetus', 'Mother'],\n",
        "            yticklabels=['Fetus', 'Mother'],\n",
        "            ax=axs[2])\n",
        "\n",
        "axs[2].set_xlabel(\"Predicted labels\", weight='bold')\n",
        "axs[2].set_ylabel(\"True labels\", weight='bold')\n",
        "axs[2].set_title(\"Confusion Matrix\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qMwFpqqG8Q57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep-ViT"
      ],
      "metadata": {
        "id": "QHkXdw5Q8cQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vit_pytorch.deepvit import DeepViT\n",
        "\n",
        "vit_model = DeepViT(\n",
        "    image_size = 128,\n",
        "    patch_size = 32,\n",
        "    num_classes = 2,\n",
        "    dim = 256,\n",
        "    depth = 6,\n",
        "    heads = 14,\n",
        "    mlp_dim = 128,\n",
        "    dropout = 0.3,\n",
        ")"
      ],
      "metadata": {
        "id": "EcKh8ELu80Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CaiT"
      ],
      "metadata": {
        "id": "tQWPly5I9On4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vit_pytorch.cait import CaiT\n",
        "\n",
        "vit_model = CaiT(\n",
        "    image_size = 128,\n",
        "    patch_size = 32,\n",
        "    num_classes = 2,\n",
        "    dim = 1024,\n",
        "    depth = 12,\n",
        "    cls_depth = 2,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.05,\n",
        ")"
      ],
      "metadata": {
        "id": "oeWsdwzZ9RTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PiT"
      ],
      "metadata": {
        "id": "nSYl20Et9kAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vit_pytorch.pit import PiT\n",
        "\n",
        "v = PiT(\n",
        "    image_size = 128,\n",
        "    patch_size = 32,\n",
        "    dim = 1024,\n",
        "    num_classes = 2,\n",
        "    depth = (3, 3, 3),\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1\n",
        ")"
      ],
      "metadata": {
        "id": "CTtpLKey9ggz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distillation Student Model"
      ],
      "metadata": {
        "id": "sllfCXMm9xue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "metadata": {
        "id": "LJi8C1pJA9Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self, num_classes=2, proj_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        effnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "        self.stem = effnet._conv_stem\n",
        "        self.bn0 = effnet._bn0\n",
        "        self.blocks = nn.Sequential(*effnet._blocks[:3])\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.classifier = nn.Linear(effnet._blocks[2]._project_conv.out_channels, num_classes)\n",
        "\n",
        "        self.proj = nn.Linear(effnet._blocks[2]._project_conv.out_channels, proj_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.bn0(x)\n",
        "        x = self.blocks(x)\n",
        "        pooled = self.pool(x).flatten(1)\n",
        "        logits = self.classifier(pooled)\n",
        "        features = self.proj(pooled)\n",
        "        return logits, features\n"
      ],
      "metadata": {
        "id": "Wo1RBoHB95_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from vit_pytorch import DeepViT\n",
        "\n",
        "class TeacherDeepViT(nn.Module):\n",
        "    def __init__(self, num_classes=2, proj_dim=256):\n",
        "        super().__init__()\n",
        "        self.deepvit = DeepViT(\n",
        "            image_size=128,\n",
        "            patch_size=32,\n",
        "            num_classes=num_classes,\n",
        "            dim=1024,\n",
        "            depth=6,\n",
        "            heads=14,\n",
        "            mlp_dim=2048\n",
        "        )\n",
        "        self.deepvit.to_logits = nn.Identity()\n",
        "        self.projector = nn.Linear(1024, proj_dim)\n",
        "        self.classifier = nn.Linear(proj_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.deepvit(x)\n",
        "        proj_feat = self.projector(feat)\n",
        "        logits = self.classifier(proj_feat)\n",
        "        return logits, proj_feat\n"
      ],
      "metadata": {
        "id": "jqWVaRVA_fgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_loss(sm_logits, sm_feat, tm_logits, tm_pos_feat, tm_neg_feat, alpha=0.2, T=2.0):\n",
        "    sm_log_probs = F.log_softmax(sm_logits, dim=1)\n",
        "    tm_probs = F.softmax(tm_logits / T, dim=1)\n",
        "    kl_loss = F.kl_div(sm_log_probs, tm_probs, reduction='batchmean')\n",
        "\n",
        "    d_pos = F.pairwise_distance(sm_feat, tm_pos_feat)\n",
        "    d_neg = F.pairwise_distance(sm_feat, tm_neg_feat)\n",
        "    triplet_loss = torch.clamp(d_pos - d_neg + alpha, min=0.0).mean()\n",
        "\n",
        "    return kl_loss + triplet_loss, kl_loss.item(), triplet_loss.item()\n"
      ],
      "metadata": {
        "id": "33FJWHEw_Dam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(student_model.parameters(), lr=3e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-5)"
      ],
      "metadata": {
        "id": "Bd0gfrKC_GQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_model = TeacherDeepViT()\n",
        "tm_model.eval()\n",
        "tm_model.to(device)\n",
        "\n",
        "student_model = StudentModel().to(device)"
      ],
      "metadata": {
        "id": "53vnW5hP_que"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "class TripletMelDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.anchor_samples = []\n",
        "        self.class_to_paths = {'mother': [], 'fetus': []}\n",
        "\n",
        "        for label in ['mother', 'fetus']:\n",
        "            paths = glob(os.path.join(root_dir, label, \"*.npy\"))\n",
        "            self.class_to_paths[label].extend(paths)\n",
        "            for p in paths:\n",
        "                self.anchor_samples.append( (p, label) )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anchor_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_path, anchor_class = self.anchor_samples[idx]\n",
        "        other_class = 'fetus' if anchor_class == 'mother' else 'mother'\n",
        "\n",
        "        anchor = np.load(anchor_path)\n",
        "        anchor = torch.tensor(anchor, dtype=torch.float32).unsqueeze(0)\n",
        "        anchor_label = 0 if anchor_class == 'mother' else 1\n",
        "\n",
        "        pos_path = random.choice(self.class_to_paths[anchor_class])\n",
        "        pos = np.load(pos_path)\n",
        "        pos = torch.tensor(pos, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        neg_path = random.choice(self.class_to_paths[other_class])\n",
        "        neg = np.load(neg_path)\n",
        "        neg = torch.tensor(neg, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        return anchor, anchor_label, pos, neg\n"
      ],
      "metadata": {
        "id": "sLedJrC8AY7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TripletMelDataset('/content/drive/MyDrive/SUFHSDB/training_data')\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_ds = TripletMelDataset('/content/drive/MyDrive/SUFHSDB/testing_data')\n",
        "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "ZpobXmbLAdJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "    student_model.train()\n",
        "    running_loss = 0.0\n",
        "    running_kl = 0.0\n",
        "    running_triplet = 0.0\n",
        "\n",
        "    for anchor, anchor_label, pos, neg in tqdm(train_loader):\n",
        "        anchor = anchor.to(device)\n",
        "        anchor_label = anchor_label.to(device)\n",
        "        pos = pos.to(device)\n",
        "        neg = neg.to(device)\n",
        "\n",
        "        def resize(x):\n",
        "            x = torch.nn.functional.interpolate(x, size=(256,256), mode='bilinear')\n",
        "            x = x.repeat(1,3,1,1)\n",
        "            return x\n",
        "        anchor = resize(anchor)\n",
        "        pos = resize(pos)\n",
        "        neg = resize(neg)\n",
        "\n",
        "        sm_logits, sm_feat = student_model(anchor)\n",
        "        with torch.no_grad():\n",
        "            tm_logits_pos, tm_pos_feat = teacher_model(pos)\n",
        "            _, tm_neg_feat = teacher_model(neg)\n",
        "\n",
        "        loss, kl, triplet = combined_loss(sm_logits, sm_feat, tm_logits_pos, tm_pos_feat, tm_neg_feat)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_kl += kl\n",
        "        running_triplet += triplet\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss={running_loss:.4f} KL={running_kl:.4f} Triplet={running_triplet:.4f}\")"
      ],
      "metadata": {
        "id": "UBq-rH8bAunA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}